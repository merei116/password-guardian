{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9d337c8",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import json\n",
    "import re\n",
    "from typing import List, Dict\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset, random_split\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import random_split\n",
    "from wordcloud import WordCloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1601a0f3",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "def seed_worker(worker_id):\n",
    "    worker_seed = SEED + worker_id\n",
    "    np.random.seed(worker_seed)\n",
    "    random.seed(worker_seed)\n",
    "\n",
    "g = torch.Generator()\n",
    "g.manual_seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebc1a6aa",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "DATA_PATH = 'dataset/maria.csv'\n",
    "BASE_MODEL_PATH = 'password_lstm.pth'\n",
    "PERSONAL_MODEL_PATH = 'personal_lstm.pth'\n",
    "PATTERN_OUTPUT_PATH = 'personal_password_patterns.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5edfc0c8",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "max_sequence_length = 50\n",
    "embedding_dim = 64\n",
    "hidden_dim = 128\n",
    "batch_size = 64\n",
    "num_epochs = 10\n",
    "learning_rate = 1e-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90e30ee2",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "class PasswordLSTM(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim):\n",
    "        super(PasswordLSTM, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers=2, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, vocab_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        x, _ = self.lstm(x)\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3d14c43",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def save_model(model, optimizer, epoch, file_path):\n",
    "    checkpoint = {\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict()\n",
    "    }\n",
    "    torch.save(checkpoint, file_path)\n",
    "    print(f\"‚úÖ –ú–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤ '{file_path}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75063a11",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def load_model(model, optimizer, file_path):\n",
    "    if os.path.exists(file_path):\n",
    "        checkpoint = torch.load(file_path)\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        start_epoch = checkpoint['epoch']\n",
    "        print(f\"‚úÖ –ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞ –∏–∑ '{file_path}' ‚Äî –ø—Ä–æ–¥–æ–ª–∂–∞–µ–º —Å —ç–ø–æ—Ö–∏ {start_epoch + 1}\")\n",
    "        return start_epoch\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è –§–∞–π–ª —Å –º–æ–¥–µ–ª—å—é –Ω–µ –Ω–∞–π–¥–µ–Ω ‚Äî –Ω–∞—á–∏–Ω–∞–µ–º –æ–±—É—á–µ–Ω–∏–µ —Å –Ω—É–ª—è.\")\n",
    "        return 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77464158",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "base_chars = set(\n",
    "    'abcdefghijklmnopqrstuvwxyz'\n",
    "    'ABCDEFGHIJKLMNOPQRSTUVWXYZ'\n",
    "    '0123456789!@#$%^&*()-_=+[]{}|;:,.<>?/\\\\'\n",
    ")\n",
    "char_to_idx = {char: idx + 1 for idx, char in enumerate(base_chars)}\n",
    "char_to_idx['<PAD>'] = 0\n",
    "char_to_idx['<UNK>'] = len(char_to_idx) + 1\n",
    "idx_to_char = {idx: char for char, idx in char_to_idx.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e729d418",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def prepare_data(passwords: List[str], char_to_idx: Dict[str, int]):\n",
    "    sequences = [\n",
    "        [char_to_idx.get(c, char_to_idx['<UNK>']) for c in pwd]\n",
    "        for pwd in passwords\n",
    "    ]\n",
    "    sequences = [\n",
    "        seq[:max_sequence_length] + [char_to_idx['<PAD>']] * (max_sequence_length - len(seq))\n",
    "        for seq in sequences\n",
    "    ]\n",
    "    X = torch.tensor(sequences, dtype=torch.long)\n",
    "    y = torch.tensor([seq[1:] + [char_to_idx['<PAD>']] for seq in sequences], dtype=torch.long)\n",
    "    dataset = TensorDataset(X, y)\n",
    "    return DataLoader(\n",
    "        dataset, batch_size=batch_size, shuffle=True,\n",
    "        worker_init_fn=seed_worker, generator=g\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b2d8c83",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "vocab_size = len(char_to_idx)\n",
    "base_model = PasswordLSTM(vocab_size, embedding_dim, hidden_dim)\n",
    "base_optimizer = optim.Adam(base_model.parameters(), lr=learning_rate)\n",
    "start_epoch = load_model(base_model, base_optimizer, BASE_MODEL_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20c339a9",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "if not os.path.exists(DATA_PATH):\n",
    "    raise FileNotFoundError(f\"‚ö†Ô∏è –§–∞–π–ª '{DATA_PATH}' –Ω–µ –Ω–∞–π–¥–µ–Ω!\")\n",
    "personal_data = pd.read_csv(DATA_PATH)['String'].tolist()\n",
    "personal_loader = prepare_data(personal_data, char_to_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63ebbd39",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "personal_model = PasswordLSTM(vocab_size, embedding_dim, hidden_dim)\n",
    "personal_optimizer = optim.Adam(personal_model.parameters(), lr=learning_rate)\n",
    "personal_model.load_state_dict(base_model.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f747214",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "dataset = personal_loader.dataset\n",
    "dataset_size = len(dataset)\n",
    "train_size = int(0.8 * dataset_size)\n",
    "val_size = dataset_size - train_size\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size], generator=g)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True,\n",
    "                          worker_init_fn=seed_worker, generator=g)\n",
    "val_loader   = DataLoader(val_dataset,   batch_size=batch_size,\n",
    "                          worker_init_fn=seed_worker, generator=g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac5c5307",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def train_and_validate(model, train_loader, val_loader, optimizer, loss_fn, num_epochs):\n",
    "    history = {'train_loss': [], 'val_loss': []}\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        # Train\n",
    "        model.train()\n",
    "        total_train = 0.0\n",
    "        for x_batch, y_batch in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(x_batch)\n",
    "            loss = loss_fn(outputs.view(-1, model.fc.out_features), y_batch.view(-1))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_train += loss.item()\n",
    "        avg_train = total_train / len(train_loader)\n",
    "        history['train_loss'].append(avg_train)\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        total_val = 0.0\n",
    "        with torch.no_grad():\n",
    "            for x_batch, y_batch in val_loader:\n",
    "                outputs = model(x_batch)\n",
    "                loss = loss_fn(outputs.view(-1, model.fc.out_features), y_batch.view(-1))\n",
    "                total_val += loss.item()\n",
    "        avg_val = total_val / len(val_loader)\n",
    "        history['val_loss'].append(avg_val)\n",
    "\n",
    "        print(f\"Epoch {epoch}/{num_epochs} ‚Äî Train Loss: {avg_train:.4f}, Val Loss: {avg_val:.4f}\")\n",
    "\n",
    "    return history\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "history = train_and_validate(\n",
    "    personal_model, train_loader, val_loader,\n",
    "    personal_optimizer, loss_fn, num_epochs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eed831d",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,5))\n",
    "plt.plot(history['train_loss'], label='Train Loss', marker='o')\n",
    "plt.plot(history['val_loss'],   label='Val Loss',   marker='o')\n",
    "plt.title('Train vs Val Loss –ø–æ —ç–ø–æ—Ö–∞–º')\n",
    "plt.xlabel('–≠–ø–æ—Ö–∞')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "856e64fa",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "save_model(personal_model, personal_optimizer, num_epochs - 1, PERSONAL_MODEL_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecb849d8",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def extract_mask(pwd: str) -> str:\n",
    "    return ''.join([\n",
    "        'X' if c.isalpha() else\n",
    "        'D' if c.isdigit() else\n",
    "        'S' if re.match(r'[!@#$%^&*()\\-_=+]', c) else '_'\n",
    "        for c in pwd\n",
    "    ])\n",
    "\n",
    "def classify_mask(mask: str) -> str:\n",
    "    if all(c == 'D' for c in mask): return \"—Ü–∏—Ñ—Ä–æ–≤–æ–π\"\n",
    "    if all(c == 'X' for c in mask): return \"—Å–ª–æ–≤–µ—Å–Ω—ã–π\"\n",
    "    if all(c == 'S' for c in mask): return \"—Å–ø–µ—Ü—Å–∏–º–≤–æ–ª—ã\"\n",
    "    if 'X' in mask and 'D' in mask and 'S' in mask: return \"–≥–∏–±—Ä–∏–¥: –±—É–∫–≤—ã + —Ü–∏—Ñ—Ä—ã + —Å–∏–º–≤–æ–ª—ã\"\n",
    "    if 'X' in mask and 'D' in mask: return \"–≥–∏–±—Ä–∏–¥: –±—É–∫–≤—ã + —Ü–∏—Ñ—Ä—ã\"\n",
    "    if 'D' in mask and 'S' in mask: return \"–≥–∏–±—Ä–∏–¥: —Ü–∏—Ñ—Ä—ã + —Å–∏–º–≤–æ–ª—ã\"\n",
    "    if 'X' in mask and 'S' in mask: return \"–≥–∏–±—Ä–∏–¥: –±—É–∫–≤—ã + —Å–∏–º–≤–æ–ª—ã\"\n",
    "    if mask.lower() != mask and mask.upper() != mask and 'X' in mask: return \"–∑–∏–≥–∑–∞–≥-—Ä–µ–≥–∏—Å—Ç—Ä\"\n",
    "    return \"–¥—Ä—É–≥–æ–µ\"\n",
    "\n",
    "results = analyze_passwords = lambda pwds: None  # placeholder if needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64478aa5",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "results = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c5b8b67",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "word_freq = results[\"words\"] if results else {}\n",
    "if word_freq:\n",
    "    wordcloud = WordCloud(width=800, height=400, background_color='white').generate_from_frequencies(word_freq)\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.imshow(wordcloud, interpolation='bilinear')\n",
    "    plt.axis('off')\n",
    "    plt.title(\"üî• –ß–∞—Å—Ç–æ –≤—Å—Ç—Ä–µ—á–∞—é—â–∏–µ—Å—è –ø–æ–¥—Å—Ç—Ä–æ–∫–∏ –≤ –ø–∞—Ä–æ–ª—è—Ö\")\n",
    "    plt.show()\n",
    "\n",
    "if results:\n",
    "    mutation_percent = float(results[\"profile\"][\"mutation_usage\"].strip('%'))\n",
    "    plt.pie(\n",
    "        [mutation_percent, 100 - mutation_percent],\n",
    "        labels=[\"–° –º—É—Ç–∞—Ü–∏—è–º–∏\", \"–ë–µ–∑ –º—É—Ç–∞—Ü–∏–π\"],\n",
    "        autopct='%1.1f%%'\n",
    "    )\n",
    "    plt.title(\"üß¨ –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –º—É—Ç–∞—Ü–∏–π –≤ –ø–∞—Ä–æ–ª—è—Ö\")\n",
    "    plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
